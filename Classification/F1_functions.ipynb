{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "from sympy import S, symbols\n",
    "from collections import Counter\n",
    "import sklearn \n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "#sns.mpl.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# Directory to store pickled dataframes\n",
    "directory = '/Users/dianaow/Documents/formula-1-race-data/dataframes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score , mean_absolute_error, mean_squared_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, accuracy_score, precision_score, average_precision_score, \\\n",
    "classification_report, recall_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.base import clone\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "import statsmodels.graphics.regressionplots \n",
    "import mord as mord\n",
    "from skmultilearn.problem_transform import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_pickle(directory, filename):\n",
    "    df = pd.DataFrame()\n",
    "    filepath = directory + filename\n",
    "    with open(filepath, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split dataset to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, scaler):\n",
    "    \n",
    "    if scaler == 'StandardScaler':\n",
    "        SS = StandardScaler()\n",
    "        Xs_train = SS.fit_transform(X_train)\n",
    "        Xs_test = SS.fit_transform(X_test)\n",
    "    elif scaler == 'MinMaxScaler':\n",
    "        mm = MinMaxScaler()\n",
    "        Xs_train = mm.fit_transform(X_train)\n",
    "        Xs_test = mm.fit_transform(X_test)  \n",
    "    elif scaler == False:\n",
    "        Xs_train = X_train\n",
    "        Xs_test = X_test\n",
    "        \n",
    "    return Xs_train, Xs_test\n",
    "\n",
    "def build_train_test_set(df, df_test, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \n",
    "                         name=None, races_curr_same_cat=None, races_same_cat=None, multilabel=False, print_stats=False):\n",
    "    \n",
    "    \"\"\"\n",
    "       Capabilites of this function:\n",
    "       1) Splits dataset to a train and test set and allows for a single or multiple target variables. \n",
    "       2) 2 methods of train-test split\n",
    "           Approach 1: Train-test split by year (set all_races=None)\n",
    "               - This is not a viable options if model includes features that are only known pre-race. eg drivers' selected tyre sets and qualifying position)\n",
    "           Approach 2: Train-test split by races\n",
    "\n",
    "       Notes:\n",
    "       - If multilabel=True, input 'target_var' and 'target_var_list' as the same variables.\n",
    "    \"\"\"\n",
    "        \n",
    "    # =============================================================================================\n",
    "    # BUILD TRAIN AND TEST SET\n",
    "    train_set = df[df['year'].isin(train_yr)].reset_index(drop=True)\n",
    "    \n",
    "    if name != None:\n",
    "\n",
    "        train_set = train_set[train_set['name'].isin(races_same_cat['name'])]\n",
    "        target_ibdex = races_curr_same_cat.index(name)\n",
    "        races_before = races_curr_same_cat[:target_ibdex]\n",
    "        addto_train_set = df[(df['year'].isin(test_yr)) & (df['name'].isin(races_before))].reset_index(drop=True)\n",
    "        train_set = train_set.append(addto_train_set)\n",
    "            \n",
    "        test_set = df_test[df_test['name'] == name].reset_index(drop=True)\n",
    "    \n",
    "    elif (name == None) & (races_same_cat == None) & (races_curr_same_cat == None):\n",
    "        test_set = df_test\n",
    "        \n",
    "    # Separate index, features and target variable\n",
    "    arr = []\n",
    "    arr = list(index_list)\n",
    "    arr_tar_tvar = list(target_var_list)\n",
    "    arr.extend(arr_tar_tvar)\n",
    "    learning_columns = np.setdiff1d(train_set.columns, np.array(arr))\n",
    "    X_train = train_set.loc[:, learning_columns]\n",
    "    X_test = test_set.loc[:, learning_columns]\n",
    "\n",
    "    if multilabel==True:\n",
    "        Y_train = train_set[target_var_list]\n",
    "        Y_test = test_set[target_var_list]\n",
    "        # Ensure target variables are in integers as alogrithms will throw an error otherwise\n",
    "        Y_train = Y_train.apply(lambda x: x.astype(int)) # Dataframe\n",
    "        Y_test = Y_test.apply(lambda x: x.astype(int))\n",
    "    else:\n",
    "        Y_train = train_set[target_var]\n",
    "        Y_test = test_set[target_var]\n",
    "        Y_train = Y_train.astype(int) # Series\n",
    "        Y_test = Y_test.astype(int)\n",
    "        \n",
    "    # =============================================================================================\n",
    "    # Standard scale the dataset after train-test split\n",
    "    Xs_train, Xs_test = scale_data(X_train, X_test, scaler)\n",
    "        \n",
    "    # =============================================================================================\n",
    "    # PRINT STATISTICS (because multilabel option will generate a dataframe, whereas a single target variable results in a series,\n",
    "    # to print statistics, different methods apply...)\n",
    "    \n",
    "    if (multilabel==True) & (print_stats==True):\n",
    "        print '=================================='\n",
    "        print 'Race:', name\n",
    "        print '=================================='\n",
    "\n",
    "        print 'Train set:' + str(len(Y_train))\n",
    "        print '---------------------------------'\n",
    "        print 'Shape of Train Set: ', Xs_train.shape\n",
    "        print Y_train.apply(pd.value_counts).fillna(0)\n",
    "\n",
    "        print '=================================='\n",
    "        print 'Test set: ' + str(len(Y_test))\n",
    "        print '---------------------------------'\n",
    "        print 'Shape of Test Set: ', Xs_test.shape\n",
    "        test_vc = Y_test.apply(pd.value_counts).fillna(0)\n",
    "        print test_vc\n",
    "\n",
    "    elif (multilabel==False) & (print_stats==True):\n",
    "        print '=================================='\n",
    "        print 'Race:', name\n",
    "        print '=================================='\n",
    "\n",
    "        print 'Train set:' + str(Y_train.count())\n",
    "        print '---------------------------------'\n",
    "        print 'Shape of Train Set: ', Xs_train.shape\n",
    "        print Y_train.value_counts() \n",
    "\n",
    "        print '=================================='\n",
    "        print 'Test set: ' + str(Y_test.count())\n",
    "        print '---------------------------------'\n",
    "        print 'Shape of Test Set: ', Xs_test.shape\n",
    "        print Y_test.value_counts() \n",
    "\n",
    "    return train_set, test_set, Xs_train, Xs_test, np.array(Y_train).ravel(), np.array(Y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to fit dataset on model and generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MODEL_FITTING(Xs_train, y_train, classifier, sampler, multilabel=False):\n",
    "    \n",
    "    \"\"\"\n",
    "       Generic function enabling fitting on dataset based on a chosen algorithm (eg. classifier) and sampler.\n",
    "       Allows a single target variable or multiple target variables as inputs (eg. multilabel classification)\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove target variables with one class only as this causes some algorithms to throw an error.\n",
    "    #if multilabel==True:\n",
    "        #col_train = [i for i in y_train if np.unique(y_train[i]).size > 1]\n",
    "        #y_train = y_train[col_train]\n",
    "        #y_test = y_test[col_train]\n",
    "\n",
    "    # If sampling required, sample first, then fit on train set\n",
    "    cXs_train_sm, cy_train_sm = sampler.fit_sample(Xs_train, y_train)\n",
    "    classifier.fit(cXs_train_sm, cy_train_sm)\n",
    "    \n",
    "    return cXs_train_sm, cy_train_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PREDICTION(train_set, test_set, Xs_train, Xs_test, y_train, y_test, \\\n",
    "               name, classifier, sampler, generator, index_list, target_var, target_var_list, \\\n",
    "               cross_validate=False, stacking=False, multilabel=False):\n",
    "    \n",
    "    # =============================================================================================\n",
    "    # Cross-validation on train set\n",
    "    if (cross_validate==True):\n",
    "        \n",
    "        pred = pd.DataFrame()\n",
    "        pred_train = pd.DataFrame()\n",
    "        proba = pd.DataFrame()\n",
    "        proba_train = pd.DataFrame()\n",
    "        cv_results = [] \n",
    "        cv_results.append(cross_val_score(classifier[1], Xs_train, y_train, \\\n",
    "                                          scoring = \"accuracy\", cv = generator, n_jobs=njobs))\n",
    "        \n",
    "        cv_means = np.mean(cv_results)\n",
    "        cv_std = np.std(cv_results)\n",
    "        \n",
    "        # Store results in dataframe\n",
    "        results_stats = pd.DataFrame({'Index': name, 'Target Var': target_var, 'Method': classifier[0], 'Resampler':sampler[0], \n",
    "                                      \"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std}, index=[0])\n",
    "\n",
    "        # Rearrange columns of dataframe\n",
    "        results_stats = results_stats[['Index', 'Target Var', 'Method', 'Resampler', \"CrossValMeans\", \"CrossValerrors\"]]\n",
    "\n",
    "    # =============================================================================================\n",
    "    # PREDICTION\n",
    "    else:\n",
    "        y_pred = classifier[1].predict(Xs_test) # Make prediction on test set \n",
    "        y_pred_train = classifier[1].predict(Xs_train) # Make prediction on train set\n",
    "        y_proba = classifier[1].predict_proba(Xs_test) # Generate prediction probabilities for test set\n",
    "        y_proba_train = classifier[1].predict_proba(Xs_train) # Generate prediction probabilities for train set\n",
    "\n",
    "        def rename_some_cols(df, suffix, col_start):\n",
    "            new_names = [(i,i+suffix) for i in df.iloc[:, col_start:].columns.values]\n",
    "            df.rename(columns = dict(new_names), inplace=True)\n",
    "            return df\n",
    "\n",
    "        def make_predictions(y, dataset, stacking, merge_index):\n",
    "\n",
    "            if stacking==True:\n",
    "                pred = pd.DataFrame(y, columns=[classifier[0] + \"_\" + str(target_var)])\n",
    "            else:\n",
    "                pred = pd.DataFrame(y).rename(columns={0: str(target_var)+'_predicted'})\n",
    "                pred['tyre'] = target_var\n",
    "                pred['sampler'] = sampler[0]\n",
    "                pred['classifier'] = classifier[0]\n",
    "\n",
    "            # Merge indexes to df of predicted results\n",
    "            if merge_index==True:\n",
    "                arr = list(index_list)\n",
    "                arr.extend(arr_tar_tvar)\n",
    "                dataset = dataset[arr].reset_index(drop=True)\n",
    "                dataset = rename_some_cols(dataset, '_actual', 3)\n",
    "                pred = pd.concat([dataset, pred], axis=1)\n",
    "\n",
    "            return pred\n",
    "\n",
    "        # Convert predicted results of both train and test set to a dataframe. \n",
    "        # If single target variable, use chosen algorithm as column name.\n",
    "        if (multilabel==False) & (stacking==False):\n",
    "            arr_tar_tvar = list([target_var]) \n",
    "            pred = make_predictions(y_pred, test_set, stacking=False, merge_index=True)\n",
    "            pred_train = make_predictions(y_pred_train, train_set, stacking=False, merge_index=True)\n",
    "            proba = make_predictions(y_proba[:,1], test_set, stacking=False, merge_index=True)\n",
    "            proba_train = make_predictions(y_proba_train[:,1], train_set, stacking=False, merge_index=True)\n",
    "\n",
    "        elif (multilabel==False) & (stacking==True):\n",
    "            arr_tar_tvar = list([target_var])  \n",
    "            pred = make_predictions(y_pred, test_set, stacking=True, merge_index=False)\n",
    "            pred_train = make_predictions(y_pred_train, train_set, stacking=True, merge_index=False)\n",
    "            proba = make_predictions(y_proba[:,1], test_set, stacking=True, merge_index=False)\n",
    "            proba_train = make_predictions(y_proba_train[:,1], train_set, stacking=True, merge_index=False)\n",
    "\n",
    "        # If multilabel, use target variables as column name.\n",
    "        else:\n",
    "            arr_tar_tvar = list(target_var_list)\n",
    "            col_train = [i for i in y_train if np.unique(y_train[i]).size > 1]\n",
    "            pred = pd.DataFrame(y_pred.todense(), columns=col_train)\n",
    "            pred['classifier'] = classifier[0]\n",
    "            pred_train = pd.DataFrame(y_pred_train.todense(), columns=col_train)\n",
    "            pred_train['classifier'] = classifier[0] \n",
    "        \n",
    "        if multilabel==False:\n",
    "            results_stats = calc_classification_stats(name, target_var, classifier[0], sampler[0], y_test, y_pred, average=None)  \n",
    "        else:\n",
    "            results_stats = calc_classification_stats(name, target_var, classifier[0], sampler[0], y_test, y_pred, average='samples')\n",
    " \n",
    "    return results_stats, pred, pred_train, proba, proba_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilabel_PREDICTION_loop(df, df_test, train_yr, test_yr, index_list, target_var_list, \\\n",
    "                                 scaler, methods, samplers, generator, \\\n",
    "                                 races_curr_same_cat, races_same_cat, all_races, \\\n",
    "                                 zV_nzV_check=False, VIF_check=False, cross_validate=False, multilabel=True, print_stats=False\\\n",
    "                                ):\n",
    "\n",
    "    results_all = pd.DataFrame()\n",
    "    pred_all = pd.DataFrame()\n",
    "    pred_train_all = pd.DataFrame()\n",
    "    proba_all = pd.DataFrame()\n",
    "    proba_train_all = pd.DataFrame()\n",
    "    \n",
    "    # =============================================================================================\n",
    "    # MULTI-LABEL CLASSIFICATION\n",
    "    if multilabel==True:\n",
    "        for m, s in itertools.product(methods, samplers):\n",
    "            for name in races_curr_same_cat:\n",
    "                train_set, test_set, Xs_train, Xs_test, Y_train, Y_test = \\\n",
    "                    build_train_test_set(\n",
    "                                          df, df_test, train_yr, test_yr, \\\n",
    "                                          index_list, target_var, target_var_list, scaler, \\\n",
    "                                          name, races_curr_same_cat, races_same_cat, \\\n",
    "                                          zV_nzV_check, VIF_check, multilabel, print_stats=False)\n",
    "                    \n",
    "                cXs_train_sm, cy_train_sm = MODEL_FITTING(Xs_train, Y_train, m[1], s[1], multilabel)\n",
    "\n",
    "                results, pred, pred_train, proba, proba_train = PREDICTION(\n",
    "                                           train_set, test_set, cXs_train_sm, Xs_test, cy_train_sm, Y_test, \\\n",
    "                                           name, m, s, generator, \\\n",
    "                                           index_list, target_var, target_var_list, \\\n",
    "                                           cross_validate, stacking, multilabel)\n",
    "\n",
    "                results_all, proba_all, proba_train_all, pred_all, pred_train_all = \\\n",
    "                    concat_dfs(results, pred, pred_train, proba, proba_train)\n",
    "                \n",
    "        return results_all, proba_all, proba_train_all, pred_all, pred_train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-label Classification (with ensemble stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singletarget_PREDICTION_loop(df, df_test, train_yr, test_yr, index_list, target_var_list, \n",
    "                                 scaler, methods, samplers, generator, races_curr_same_cat, races_same_cat, \n",
    "                                 meta_learner, cross_validate=False, multilabel=False, \n",
    "                                 ensemble=False, ensb_use_all=False, print_stats=False \n",
    "                            ):\n",
    "\n",
    "    results_all = pd.DataFrame()\n",
    "    pred_all = pd.DataFrame()\n",
    "    pred_train_all = pd.DataFrame()\n",
    "    proba_all = pd.DataFrame()\n",
    "    proba_train_all = pd.DataFrame()\n",
    "    Pstrain = pd.DataFrame()\n",
    "    Pstest = pd.DataFrame()\n",
    "                                    \n",
    "    # =============================================================================================\n",
    "    # SINGLE TARGET VARIABLE\n",
    "    for target_var in target_var_list:\n",
    "        \n",
    "        P_train_all = pd.DataFrame()\n",
    "        P_test_all = pd.DataFrame()\n",
    "\n",
    "        for idx,name in enumerate(races_curr_same_cat):\n",
    "        \n",
    "            # Initialize new dataframe for each race\n",
    "            P_train_race = pd.DataFrame()\n",
    "            P_test_race = pd.DataFrame()\n",
    "        \n",
    "            for m, s in itertools.product(methods[idx], samplers):\n",
    "                \n",
    "                if ensemble==True:\n",
    "                \n",
    "                    train_set, test_set, results, pred, pred_train, meta_features_test, meta_features_train = \\\n",
    "                        fit_and_predict(m, s, generator, df, df_test, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \\\n",
    "                            name, races_curr_same_cat, races_same_cat, results_all, proba_all, proba_train_all, pred_all, pred_train_all, \\\n",
    "                            multilabel, cross_validate, print_stats, stacking=True, concat=False\n",
    "                        )\n",
    "\n",
    "                    # Utilize prediction probabilities as meta features\n",
    "                    # Generates prediction matrix of meta features -> With each loop, store the new meta feature generated in the step above.\n",
    "                    # By the end of the nested loop's iteration, this df should store the meta features of ONE target var from all models.\n",
    "                    P_train_race = pd.concat([P_train_race, meta_features_train], axis=1)\n",
    "                    P_test_race = pd.concat([P_test_race, meta_features_test], axis=1)\n",
    "\n",
    "                    if ensb_use_all==True:\n",
    "                        # Merge rest of features to the prediction matrix \n",
    "                        if (len(P_train_race.columns) == len(methods[0])) & (len(P_test_race.columns) == len(methods[0])):\n",
    "                            P_train_race = pd.concat([train_set.reset_index(drop=True), P_train_race.reset_index(drop=True)], axis=1)\n",
    "                            P_test_race = pd.concat([test_set.reset_index(drop=True), P_test_race.reset_index(drop=True)], axis=1)\n",
    "\n",
    "                    else:\n",
    "                        # Only merge index to the prediction matrix (Do not merge all features)\n",
    "                        if (len(P_train_race.columns) == len(methods[0])) & (len(P_test_race.columns) == len(methods[0])):\n",
    "                            arr = list(index_list)\n",
    "                            arr.extend([target_var])\n",
    "                            P_train_race = pd.concat([train_set[arr].reset_index(drop=True), P_train_race.reset_index(drop=True)], axis=1)\n",
    "                            P_test_race = pd.concat([test_set[arr].reset_index(drop=True), P_test_race.reset_index(drop=True)], axis=1)\n",
    "                \n",
    "                else:     \n",
    "                    results_all, pred_all, pred_train_all, proba_all, proba_train_all = \\\n",
    "                        fit_and_predict(m, s, generator, df, df_test, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \\\n",
    "                            name, races_curr_same_cat, races_same_cat, results_all, proba_all, proba_train_all, pred_all, pred_train_all, \\\n",
    "                            multilabel, cross_validate, print_stats, stacking=False, concat=True\n",
    "                        )\n",
    "                        \n",
    "            if ensemble==True:\n",
    "\n",
    "                results_all, pred_all, pred_train_all, proba_all, proba_train_all = \\\n",
    "                    fit_and_predict(meta_learner, samplers[0], generator, P_train_race, P_test_race, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \\\n",
    "                        name, races_curr_same_cat, races_same_cat, results_all, proba_all, proba_train_all, pred_all, pred_train_all,\\\n",
    "                        multilabel, cross_validate, print_stats, stacking=False, concat=True\n",
    "                    )\n",
    "                    \n",
    "                Pstrain = pd.concat([Pstrain, P_train_race])  \n",
    "                Pstest = pd.concat([Pstest, P_test_race])  \n",
    "                \n",
    "    if ensemble==True:\n",
    "        return results_all, pred_all, pred_train_all, Pstest, Pstrain\n",
    "    else:\n",
    "        return results_all, pred_all, pred_train_all, proba_all, proba_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(m, s, generator, df, df_test, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \\\n",
    "                    name, races_curr_same_cat, races_same_cat, results_all, proba_all, proba_train_all, pred_all, pred_train_all,\\\n",
    "                    multilabel=False, cross_validate=False, print_stats=False, stacking=False, concat=False\n",
    "                    ):\n",
    "    \n",
    "    train_set, test_set, Xs_train, Xs_test, Y_train, Y_test = build_train_test_set(\n",
    "                              df, df_test, train_yr, test_yr, index_list, target_var, target_var_list, scaler, \\\n",
    "                              name, races_curr_same_cat, races_same_cat, multilabel, print_stats)\n",
    "\n",
    "    cXs_train_sm, cy_train_sm = MODEL_FITTING(Xs_train, Y_train, m[1], s[1], multilabel)\n",
    "\n",
    "    # This only generates ONE columns/meta feature of one target variable predicted with ONE model (eg. logreg)\n",
    "    results, pred, pred_train, proba, proba_train = PREDICTION(\n",
    "                               train_set, test_set, cXs_train_sm, Xs_test, cy_train_sm, Y_test, \\\n",
    "                               name, m, s, generator, index_list, target_var, target_var_list, \\\n",
    "                               cross_validate, stacking, multilabel)\n",
    "\n",
    "    if concat==True:\n",
    "        results_all = pd.concat([results_all, results]) # Df containing performance results of each estimator\n",
    "        proba_all = pd.concat([proba_all, proba]) # Prediction probabilities generated from test set\n",
    "        proba_train_all = pd.concat([proba_train_all, proba_train]) # Prediction probabilities generated from train set\n",
    "        pred_all = pd.concat([pred_all, pred]) # Predictions generated from test set\n",
    "        pred_train_all = pd.concat([pred_train_all, pred_train]) # Predictions generated from train set\n",
    "\n",
    "        return results_all, pred_all, pred_train_all, proba_all, proba_train_all\n",
    "    else:\n",
    "        return train_set, test_set, results, pred, pred_train, proba, proba_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to evaluation prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_classification_stats(name, target_var, classifier, sampler, y_test, y_pred, average):\n",
    "\n",
    "    # Initiate lists for storing results\n",
    "    recall = []\n",
    "    precision = []\n",
    "    baseline_accuracy = []\n",
    "    test_accuracy = []\n",
    "    f1 = []\n",
    "    avg_precision = []\n",
    "    \n",
    "    #recall.append(recall_score(y_test, y_pred, average=None))\n",
    "    #precision.append(precision_score(y_test, y_pred, average=None))\n",
    "    baseline_accuracy.append(float(pd.Series(y_test).value_counts().max()) / pd.Series(y_test).count())\n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    avg_precision.append(average_precision_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average=None)) \n",
    "\n",
    "     \n",
    "    results_stats = pd.DataFrame({'Index': str(name), 'Target Var': str(target_var), 'Method': classifier, 'Resampler':sampler,\\\n",
    "                                  'Avg Precision': avg_precision, 'F1 Score': f1, 'Test accuracy': test_accuracy, \\\n",
    "                                  'Baseline accuracy': baseline_accuracy})\n",
    "\n",
    "    results_stats = results_stats[['Index', 'Target Var', 'Method', 'Resampler', \"Baseline accuracy\", \"Test accuracy\", 'F1 Score', 'Avg Precision']]\n",
    "\n",
    "    return results_stats  \n",
    "\n",
    "def format_stats_df(df, cols_w_array):\n",
    "    \n",
    "    df_new = pd.DataFrame()\n",
    "    df_new_all = pd.DataFrame()\n",
    "    \n",
    "    for i in cols_w_array:\n",
    "        df_new = pd.DataFrame(df[i].values.tolist(),columns=[str(i)+' (minority)', str(i)+' (majority)']).reset_index(drop=True)\n",
    "\n",
    "        df_new_all = pd.concat([df_new_all, df_new], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    df_new = pd.concat([df.reset_index(drop=True), df_new_all], axis=1)\n",
    "    \n",
    "    df_new['Distance from baseline'] = df_new['Test accuracy'] - df_new['Baseline accuracy']\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def plot_algo_results(df, grp_col, metrics_list, sort_method):\n",
    "    \n",
    "    \"\"\"Function to plot statistics of prediction results\"\"\"\n",
    "\n",
    "    def calc(df, grp_col, col, new_col_name):\n",
    "        if new_col_name == 'Std':\n",
    "            df = pd.DataFrame(df.groupby([grp_col])[col].apply(lambda x: np.std(x)))\n",
    "  \n",
    "        elif new_col_name == 'Mean':\n",
    "            df = pd.DataFrame(df.groupby([grp_col])[col].apply(lambda x: np.mean(x)))\n",
    "            \n",
    "        return df.reset_index().rename(columns={col: col+ \" (\" + new_col_name + \")\"})\n",
    "    \n",
    "    # This function only calculates mean and standard deviation\n",
    "    def create_grp_stats(df, grp_col, col):\n",
    "        p_std = calc(df, grp_col, col, 'Std')\n",
    "        p_mean = calc(df, grp_col, col, 'Mean')\n",
    "        p = pd.merge(p_mean, p_std, on=grp_col, how='left')\n",
    "        return p\n",
    "    \n",
    "    # Set order \n",
    "    def sort_order(df, col_to_sortby, grp_col):\n",
    "        if sort_method == \"desc\":\n",
    "            ordering = df.sort_values([col_to_sortby])[grp_col].unique()\n",
    "        elif sort_method == \"race\":\n",
    "            ordering = races\n",
    "        else:\n",
    "            raise ValueError(\"Only desc or race are accepted keywords for sort_method variable\")\n",
    "\n",
    "        ids = reversed(list(ordering))\n",
    "        ids = [str(item) for item in ids]  \n",
    "        return ids\n",
    "        \n",
    "    # plot results\n",
    "    def plot_barplot(df, x, y, ids, row, col):\n",
    "        plt.figure()   \n",
    "        g = sns.barplot(x, y, data = df, order=ids, palette=\"Set3\", orient = \"h\", ax=axes[row][col])\n",
    "        g.set_title(x, fontsize=16)\n",
    "        for p in g.patches:\n",
    "            width = p.get_width()\n",
    "            g.text(width*1.05, p.get_y()+0.55*p.get_height(), '{:1.2f}'.format(width), ha='center', va='center')\n",
    "        \n",
    "    df_new = pd.DataFrame()\n",
    "    for i in metrics_list:\n",
    "        p = create_grp_stats(df, grp_col, i) \n",
    "        df_new = pd.concat([df_new, p], axis=1)\n",
    "        df_new = df_new.T.drop_duplicates().T\n",
    "    \n",
    "    if len(df_new.columns) == 3:\n",
    "        nrows = 1\n",
    "        ncols = 2\n",
    "    else:\n",
    "        nrows = len(df_new.columns)-len(metrics_list)-1\n",
    "        ncols = 2\n",
    "\n",
    "    to_plot = df_new.columns[1:]\n",
    "    fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15), squeeze=False)\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    counter = 0\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            ids = sort_order(df_new, to_plot[counter], grp_col)\n",
    "            plot_barplot(df_new, to_plot[counter], grp_col, ids, row, col)\n",
    "            counter += 1\n",
    "    plt.tight_layout()\n",
    "            \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper_loop(df, df_test, train_yr, test_yr, index_list, target_var_list, \n",
    "                             scaler, races_to_use, races_curr_same_cat, races_same_cat, \n",
    "                             clf, param_grid, scorer, refit, print_stats=False, plot_params=False\n",
    "                            ):\n",
    "\n",
    "    search_results_all = pd.DataFrame()\n",
    "    \n",
    "    for name in races_to_use:\n",
    "        \n",
    "        train_set, test_set, Xs_train, Xs_test, Y_train, Y_test = \\\n",
    "          build_train_test_set(\n",
    "                            df, df_test, train_yr, test_yr, \n",
    "                            index_list, target_var_list, target_var_list, scaler, \n",
    "                            name, races_curr_same_cat, races_same_cat, multilabel=False, print_stats=False)\n",
    "            \n",
    "        Y_train_flip = np.logical_not(Y_train).astype(int)\n",
    "        Y_train_flip = np.array(Y_train_flip).ravel()\n",
    "        \n",
    "        if plot_params==True:\n",
    "            plot_indiv_params(Xs_train, Y_train_flip, clf, param_grid, scorer, refit)\n",
    "        else:\n",
    "            search_results = search_all_params(name, Xs_train, Y_train_flip, clf, param_grid, scorer, refit)\n",
    "            search_results_all = pd.concat([search_results_all, search_results])\n",
    "            \n",
    "    return search_results_all\n",
    "\n",
    "def plot_indiv_params(Xs_train, Y_train_flip, clf, param_grid, scorer, refit):\n",
    "    \n",
    "    index = 1\n",
    "    plt.figure(figsize=(16,12))\n",
    "    for parameter, param_range in dict.items(param_grid):   \n",
    "        grid_search = GridSearchCV(clf, param_grid = {parameter: param_range}, scoring=scorer, refit=refit, cv=skfold, return_train_score=True, n_jobs=-1)\n",
    "        grid_search.fit(Xs_train, Y_train_flip)\n",
    "\n",
    "        df = {}\n",
    "        for i, score in enumerate(grid_search.grid_scores_):\n",
    "            df[score[0][parameter]] = score[1]\n",
    "\n",
    "        df = pd.DataFrame.from_dict(df, orient='index')\n",
    "        df.reset_index(level=0, inplace=True)\n",
    "        df = df.sort_values(by='index')\n",
    "\n",
    "        plt.subplot(3,2,index)\n",
    "        plot = plt.plot(df['index'], df[0])\n",
    "        plt.title(parameter)\n",
    "        index += 1\n",
    "    \n",
    "    return\n",
    "\n",
    "def search_all_params(name, Xs_train, Y_train_flip, clf, param_grid, scorer, refit):\n",
    "\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorer, refit=refit, cv=skfold, return_train_score=True, n_jobs=-1)\n",
    "    grid_result = grid_search.fit(Xs_train, Y_train_flip)\n",
    "\n",
    "    search_results = pd.DataFrame(grid_search.grid_scores_)\n",
    "    search_results['name'] = name\n",
    "    search_results['scorer'] = scorer\n",
    "    search_results['classifiers'] = [clone(clf.set_params(**i)) for i in search_results['parameters']]\n",
    "    search_results['best score'] = grid_search.best_score_\n",
    "    search_results = search_results.sort_values('mean_validation_score', ascending=False).head(5)\n",
    "\n",
    "    return search_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine predictors with near zero or zero variance (Required for regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearZeroVariance(X, freqCut = 95 / 5, uniqueCut = 5):\n",
    "    '''\n",
    "    Determine predictors with near zero or zero variance.\n",
    "    Inputs:\n",
    "    X: pandas data frame\n",
    "    freqCut: the cutoff for the ratio of the most common value to the second most common value\n",
    "    uniqueCut: the cutoff for the percentage of distinct values out of the number of total samples\n",
    "    Returns a tuple containing a list of column names: (zeroVar, nzVar)\n",
    "    '''\n",
    "\n",
    "    colNames = X.columns.values.tolist()\n",
    "    freqRatio = dict()\n",
    "    uniquePct = dict()\n",
    "\n",
    "    for names in colNames:\n",
    "        counts = (\n",
    "            (X[names])\n",
    "            .value_counts()\n",
    "            .sort_values(ascending = False)\n",
    "            .values\n",
    "            )\n",
    "\n",
    "        if len(counts) == 1:\n",
    "            freqRatio[names] = -1\n",
    "            uniquePct[names] = (float(len(counts)) / len(X[names])) * 100\n",
    "            continue\n",
    "\n",
    "        freqRatio[names] = counts[0] / counts[1]\n",
    "        uniquePct[names] = (float(len(counts)) / len(X[names])) * 100\n",
    "\n",
    "    zeroVar = list()\n",
    "    nzVar = list()\n",
    "    for k in uniquePct.keys():\n",
    "        if freqRatio[k] == -1:\n",
    "            zeroVar.append(k)\n",
    "\n",
    "        if uniquePct[k] < uniqueCut and freqRatio[k] > freqCut:\n",
    "            nzVar.append(k)\n",
    "\n",
    "    return(zeroVar, nzVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zV_nzV_filter(df, index_list, target_var_list):\n",
    "     \n",
    "    # IMPORTANT! copy the index_list variable or else it will be modified \n",
    "    idxes = list(index_list)\n",
    "    \n",
    "    # Select predictor columns\n",
    "    no_check_cols = [c for c in df.columns if c in target_var_list]\n",
    "    no_check_cols.extend(idxes)\n",
    "    learning_columns = np.setdiff1d(df.columns, np.array(no_check_cols))\n",
    "    df_tocheck = df.loc[:, learning_columns]\n",
    "    \n",
    "    # Check for predictors with near zero or zero variance    \n",
    "    zeroVar, nzVar = nearZeroVariance(df_tocheck)\n",
    "    zero_nz_Var = list(zeroVar)\n",
    "    zero_nz_Var.extend(nzVar)\n",
    "    \n",
    "    # Final list columns that have passed the check\n",
    "    non_zero_nz_Var = list(filter(lambda x: x not in zero_nz_Var, df_tocheck.columns.tolist()))\n",
    "    \n",
    "    # Append original index and target variable\n",
    "    non_zero_nz_Var.extend(idxes)\n",
    "    non_zero_nz_Var.extend(target_var_list)\n",
    "    \n",
    "    # Final dataframe of selected columns\n",
    "    #df = df[non_zero_nz_Var]\n",
    "    \n",
    "    #print '-------------------------------------------------------------'\n",
    "    #print 'Columns which pass low variance test:' + str(non_zero_nz_Var)\n",
    "    \n",
    "    return non_zero_nz_Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zV_nzV_filter(df, index_list, target_var_list):\n",
    "     \n",
    "    # IMPORTANT! copy the index_list variable or else it will be modified \n",
    "    idxes = list(index_list)\n",
    "    \n",
    "    # Select predictor columns\n",
    "    no_check_cols = [c for c in df.columns if c in target_var_list]\n",
    "    no_check_cols.extend(idxes)\n",
    "    learning_columns = np.setdiff1d(df.columns, np.array(no_check_cols))\n",
    "    df_tocheck = df.loc[:, learning_columns]\n",
    "    \n",
    "    # Check for predictors with near zero or zero variance    \n",
    "    zeroVar, nzVar = nearZeroVariance(df_tocheck)\n",
    "    zero_nz_Var = list(zeroVar)\n",
    "    zero_nz_Var.extend(nzVar)\n",
    "    \n",
    "    # Final list columns that have passed the check\n",
    "    non_zero_nz_Var = list(filter(lambda x: x not in zero_nz_Var, df_tocheck.columns.tolist()))\n",
    "    \n",
    "    # Append original index and target variable\n",
    "    non_zero_nz_Var.extend(idxes)\n",
    "    non_zero_nz_Var.extend(target_var_list)\n",
    "    \n",
    "    # Final dataframe of selected columns\n",
    "    #df = df[non_zero_nz_Var]\n",
    "    \n",
    "    #print '-------------------------------------------------------------'\n",
    "    #print 'Columns which pass low variance test:' + str(non_zero_nz_Var)\n",
    "    \n",
    "    return non_zero_nz_Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VIF_filter(df, index_list, target_var_list):\n",
    "    \n",
    "    # IMPORTANT! copy the index_list variable or else it will be modified \n",
    "    idxes = list(index_list)\n",
    "    \n",
    "    # Select predictor columns\n",
    "    no_check_cols = [c for c in df.columns if c in target_var_list]\n",
    "    no_check_cols.extend(idxes)\n",
    "    learning_columns = np.setdiff1d(df.columns, np.array(no_check_cols))\n",
    "    df_tocheck = df.loc[:, learning_columns]\n",
    "             \n",
    "    # Check for Variance Inflation Factor\n",
    "    # For each X, calculate VIF and save in dataframe\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(df_tocheck.values, i) for i in range(df_tocheck.shape[1])]\n",
    "    vif[\"features\"] = df_tocheck.columns\n",
    "    \n",
    "     # Final list columns that have passed the check\n",
    "    features_vif_below10 = vif[vif['VIF Factor'] < 10].features.tolist()\n",
    "    \n",
    "    # Append original index and target variable\n",
    "    features_vif_below10.extend(idxes)\n",
    "    features_vif_below10.extend(target_var_list)\n",
    "\n",
    "    # Final dataframe of selected columns\n",
    "    #df = df[features_vif_below10]\n",
    "    \n",
    "    #print '------------------------------------------------------------'\n",
    "    #print 'Columns which pass VIF test:' + str(features_vif_below10)\n",
    "        \n",
    "    return features_vif_below10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
