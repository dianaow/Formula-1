{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "import os\n",
    "import sklearn \n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "# Directory to store pickled dataframes\n",
    "directory = '/Users/dianaow/Documents/formula-1-race-data/dataframes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Imputer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, make_scorer, accuracy_score, precision_score, average_precision_score, \\\n",
    "classification_report, recall_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_pickle(directory, filename):\n",
    "    df = pd.DataFrame()\n",
    "    filepath = directory + filename\n",
    "    with open(filepath, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_list = ['year', 'name', 'driverRef']\n",
    "target_var_list = ['statusId']\n",
    "\n",
    "# List of Formula 1 races in a season in chronological order\n",
    "races = ['Australian Grand Prix',\n",
    "         'Chinese Grand Prix',\n",
    "         'Bahrain Grand Prix',\n",
    "         'Russian Grand Prix',\n",
    "         'Spanish Grand Prix',\n",
    "         'Monaco Grand Prix',\n",
    "         'Canadian Grand Prix',\n",
    "         'Azerbaijan Grand Prix',\n",
    "         'Austrian Grand Prix',\n",
    "         'British Grand Prix',\n",
    "         'Hungarian Grand Prix',\n",
    "         'Belgian Grand Prix',\n",
    "         'Italian Grand Prix',\n",
    "         'Singapore Grand Prix',\n",
    "         'Malaysian Grand Prix',\n",
    "         'Japanese Grand Prix',\n",
    "         'United States Grand Prix',\n",
    "         'Mexican Grand Prix',\n",
    "         'Brazilian Grand Prix',\n",
    "         'Abu Dhabi Grand Prix']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(\"/Users/dianaow/Downloads/F1_Tyre_data.xlsx\")\n",
    "xl.sheet_names\n",
    "pirelli = xl.parse(\"Sheet7\")\n",
    "pirelli = pirelli[pirelli['name'] != 'German Grand Prix']\n",
    "\n",
    "# Important to ensure dataframe is sorted by the F1 race calendar in chronological order to ensure dataset is filtered accurately.\n",
    "sorterIndex = dict(zip(races,range(len(races))))\n",
    "pirelli['name_Rank'] = pirelli['name'].map(sorterIndex)\n",
    "pirelli.sort_values(['year','name_Rank'], ascending = True, inplace = True) \n",
    "pirelli.drop('name_Rank', 1, inplace = True)\n",
    "races_dict = pirelli[['year', 'name']].to_dict('list')\n",
    "\n",
    "kfold_3 = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to build training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_train_test_set():\n",
    "    \n",
    "    def __init__(self, actual, df, df_test, name, index_list=index_list, target_var_list=target_var_list, \n",
    "                 races_dict=races_dict, scaler='StandardScaler'): \n",
    "\n",
    "        \"\"\"\n",
    "        Splits dataset to a train and test set  \n",
    "        A train and test set are input variables, but this function allows both sets to be scoped down further according to criteria.\n",
    "        \n",
    "        2 methods of train-test split\n",
    "           Approach 1: Train-test split by year \n",
    "               - When initializing class, set name = None\n",
    "               - This is not a viable options if model includes features that are only known pre-race. eg drivers' selected tyre sets and qualifying position)\n",
    "           Approach 2: Train-test split by races\n",
    "               - When initializing class, specify the name variable with a race\n",
    "\n",
    "           Notes:\n",
    "           -If you want to filter only races that belong to same category of pirelli assigned tyre combis, ensure the races_dict variable itself only contains the races that fall within the category\n",
    "        \n",
    "        df - dataframe containing train set\n",
    "        df_test - dataframe containing test set\n",
    "        train_yr - specify the scope of the training set. Format should be a list eg. [2015, 2016]\n",
    "                 - For eg, if train_yr=[2016], then only \n",
    "        test_yr - specify the scope of the test set. Format should be a list eg. [2017]\n",
    "        qty_races_tofilter - specify the method to scope down train set further with only selected races.\n",
    "                             For eg, if input is 4, then only the 4 previous races before the currnt race to be tested on will be included in training set. \n",
    "        races_dict - A dictionary of the F1 race calendar for multiple seasons (the year of the season and the corresponding races)\n",
    "        name - race to be tested on\n",
    "        \n",
    "        Example of parameter input:\n",
    "        train_yr = [2015, 2016], test_yr = [2017],  qty_races_tofilter= 4, name = 'Singapore Grand Prix\"\n",
    "        scope of races included in train set: \n",
    "         - 2015 Singapore Grand Prix\n",
    "         - 2016 Singapore Grand Prix\n",
    "         - 2017 British, Hungarian, Belgian, Italian Grand Prix (These are the 4 races that took place before the 2017 Singapore GP)\n",
    "        \"\"\"    \n",
    "        self.df = df\n",
    "        self.df_test = df_test       \n",
    "        self.index_list = index_list\n",
    "        self.target_var_list = target_var_list\n",
    "        self.race_dict = races_dict\n",
    "        self.name = name\n",
    "        self.scaler = scaler\n",
    "        self.actual = actual\n",
    "        \n",
    "    def train_test_split(self, train_yr, test_yr, qty_races_tofilter):\n",
    "\n",
    "        train_set = self.df[self.df['year'].isin(train_yr)].reset_index(drop=True) # Scope down training set by year, if required.\n",
    "\n",
    "        if self.name != None:\n",
    "\n",
    "            if (isinstance(qty_races_tofilter, str)) or (isinstance(qty_races_tofilter, float)):\n",
    "                raise ValueError('qty_races_tofilter variable can only be an integer, None or 0. If None is the input, then all races in train set will be included. If 0/False is the input, only races of the same name are selected.')\n",
    "\n",
    "            elif (isinstance(qty_races_tofilter, int)):\n",
    "                r = pd.DataFrame(races_dict['name'])\n",
    "                \n",
    "                index = r[r[0] == self.name].index.tolist()[-1] # Only extract the index number of the current race. Since the df is already sorted in chronological order, [-1] picks out the index of the race to be tested on.\n",
    "                train_set = pd.DataFrame()\n",
    "                # Extract a pool of past races to include in training set\n",
    "                for k,v in zip(races_dict['name'][index-qty_races_tofilter:index], races_dict['year'][index-qty_races_tofilter:index]):\n",
    "                    f = self.df[(self.df['year'] == v) & (self.df['name'] == k)]\n",
    "                    train_set = pd.concat([train_set, f])\n",
    "\n",
    "                addto_train_set = self.df[(self.df['year'].isin(train_yr)) & (self.df['name']== self.name)].reset_index(drop=True)\n",
    "                train_set = train_set.append(addto_train_set)\n",
    "\n",
    "            elif (qty_races_tofilter==None):\n",
    "                # Extract all past races to include in training set\n",
    "                races_list = [x for i, x in enumerate(races_dict['name']) if races_dict['name'].index(x) == i] # List of Formula 1 races in a season in chronological order\n",
    "                target_ibdex = races_list.index(self.name)\n",
    "                races_before = races_list[:target_ibdex]\n",
    "                addto_train_set = self.df[(self.df['year'].isin(test_yr)) & (self.df['name'].isin(races_before))].reset_index(drop=True) \n",
    "                train_set = train_set.append(addto_train_set)\n",
    "\n",
    "            test_set = self.df_test[self.df_test['name'] == self.name].reset_index(drop=True) # Only select the test set of the race to test on\n",
    " \n",
    "        elif self.name == None:\n",
    "            test_set = self.df_test\n",
    "\n",
    "        train_set = train_set.reset_index(drop=True)\n",
    "        test_set = test_set.reset_index(drop=True)\n",
    "        \n",
    "        # Separate index, features and target variable\n",
    "        learning_columns = np.setdiff1d(train_set.columns, self.index_list+self.target_var_list)\n",
    "        X_train = train_set.loc[:, learning_columns]\n",
    "        X_test = test_set.loc[:, learning_columns]\n",
    "        Y_train = np.array(train_set[self.target_var_list[0]]).ravel()\n",
    "\n",
    "        # Apply a scaler on data\n",
    "        Xs_train, Xs_test = self.scale_data(X_train, X_test, self.scaler)\n",
    "        \n",
    "        if self.actual==False:\n",
    "            Y_test = np.array(test_set[self.target_var_list[0]]).ravel()\n",
    "        else:\n",
    "            Y_test = []\n",
    "            \n",
    "        return train_set, test_set, Xs_train, Xs_test, Y_train, Y_test\n",
    "\n",
    "    def scale_data(self, X_train, X_test, scaler):\n",
    "    \n",
    "        if scaler == 'StandardScaler':\n",
    "            SS = StandardScaler()\n",
    "            Xs_train = SS.fit_transform(X_train)\n",
    "            Xs_test = SS.fit_transform(X_test)\n",
    "        elif scaler == 'MinMaxScaler':\n",
    "            mm = MinMaxScaler()\n",
    "            Xs_train = mm.fit_transform(X_train)\n",
    "            Xs_test = mm.fit_transform(X_test)  \n",
    "        elif scaler == False:\n",
    "            Xs_train = X_train\n",
    "            Xs_test = X_test\n",
    "\n",
    "        return Xs_train, Xs_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class containing functions to perform classification on one race with a single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyOneClf:\n",
    "    \n",
    "    def __init__(self, name, train_set, test_set, Xs_train, Xs_test, Y_train, Y_test, clf, generator=kfold_3,\n",
    "                 index_list=index_list, target_var_list=target_var_list):\n",
    "        \n",
    "        \"\"\"\n",
    "        This is a general purpose class containing functions enabling predictions on an array of features\n",
    "        using a single classifier, with classification results and classifier performance statistics stored in dataframes.\n",
    "\n",
    "        Parameters:\n",
    "        classifier - Must be in format[title, estimator()] eg: ['Random Forest', RandomForestClassifier()]\n",
    "        generator - Cross-validator method to split data in train/test sets.\n",
    "        train_set - dataframe of training set (including index and target variable columns)\n",
    "        test_set  - dataframe of test set (including index and target variable columns)\n",
    "        Xs_train - array of training set features\n",
    "        Xs_test - array of test set features\n",
    "        Y_train - array of training set actual target variable values\n",
    "        Y_test - array of test set actual target variable values\n",
    "        index_list - row identification (eg. year=2016, driverRef)\n",
    "        target_var_list - list containing target variables\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.Xs_train = Xs_train \n",
    "        self.Xs_test = Xs_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        self.clf = clf\n",
    "        self.generator = generator\n",
    "        self.index_list = index_list\n",
    "        self.target_var_list = target_var_list\n",
    "        \n",
    "    def feature_selection(self, fsel_list):\n",
    "        \"\"\"\n",
    "        Creates a feature-selection-classifier pipeline\n",
    "        \n",
    "        classifier - Input variable must be in the format: ['LDA', LinearDiscriminantAnalysis()]\n",
    "        name - Row identification (eg. Australian Grand Prix)\n",
    "        fsel_list - pass a list of the number of features to gridsearch (eg. [3,4,5])\n",
    "        Note: Feature selection can be computationally expensive, be careful when setting the param grid before running the function.\n",
    "        \"\"\"\n",
    "        sfs = SequentialFeatureSelector(self.clf[1],\n",
    "                                        k_features=3,\n",
    "                                        forward=False, \n",
    "                                        floating=False, \n",
    "                                        scoring='roc_auc',\n",
    "                                        verbose=0,\n",
    "                                        cv=3)\n",
    "\n",
    "        pipe = Pipeline([('sfs', sfs),\n",
    "                         (self.clf[0], self.clf[1])\n",
    "                        ])\n",
    "\n",
    "        param_grid = [\n",
    "          {'sfs__k_features': fsel_list}\n",
    "        ]\n",
    "\n",
    "        gs = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=param_grid, \n",
    "                          scoring='roc_auc', \n",
    "                          n_jobs=-1, \n",
    "                          cv=3,  \n",
    "                          refit=True)\n",
    "\n",
    "        # run gridearch\n",
    "        gs = gs.fit(self.Xs_train, self.Y_train)\n",
    "        feature_subset = gs.best_estimator_.steps[0][1].k_feature_idx_\n",
    "        Xs_train_sfs = self.Xs_train[:, feature_subset]\n",
    "        Xs_test_sfs = self.Xs_test[:, feature_subset]\n",
    "\n",
    "        df_fea_sel = pd.DataFrame({\"Method\":self.clf[0], \"Index\": self.name, 'Best score:': gs.best_score_,\n",
    "                                  'Best features:': [feature_subset]})\n",
    "        \n",
    "        return df_fea_sel, Xs_train_sfs,  Xs_test_sfs\n",
    "    \n",
    "\n",
    "    def VIF_filter(self):\n",
    "        \"\"\"\n",
    "        Check for Variance Inflation Factor of features in train set \n",
    "        \"\"\"\n",
    "        # Convert array to dataframe so that it can be passed to the vif function\n",
    "        df = pd.DataFrame(self.Xs_train)\n",
    "        df_test = pd.DataFrame(self.Xs_test)\n",
    "        \n",
    "        # For each X, calculate VIF and save in dataframe\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "        vif[\"features\"] = df.columns\n",
    "\n",
    "         # Final list columns that have passed the check\n",
    "        features_vif_below10 = vif[vif['VIF Factor'] < 10].features.tolist()\n",
    "\n",
    "        # Final dataframe of selected columns\n",
    "        df = df[features_vif_below10]\n",
    "        df_test = df_test[features_vif_below10]\n",
    "        \n",
    "        #print 'Columns which pass VIF test:' + str(features_vif_below10)\n",
    "\n",
    "        return df, df_test\n",
    "\n",
    "    \n",
    "    def cross_validate_one_clf(self):\n",
    "        \"\"\"\n",
    "        Perform cross-validation on train set using a classifier.\n",
    "        \"\"\"\n",
    "        cv_results = [] \n",
    "        proba_train = pd.DataFrame()\n",
    "        \n",
    "        for i, (train_idx, test_idx) in enumerate(self.generator.split(self.Xs_train, self.Y_train)):\n",
    "\n",
    "            fold_xtrain, fold_ytrain = self.Xs_train[train_idx, :], self.Y_train[train_idx]\n",
    "            fold_xtest, fold_ytest = self.Xs_train[test_idx, :], self.Y_train[test_idx]\n",
    "            classifier = self.fit(fold_xtrain, fold_ytrain) # Train classifier on each fold\n",
    "\n",
    "            # Generate prediction probabilites of each fold, which is then stacked to form full df of train set predictions \n",
    "            fold_Pa = classifier.predict_proba(fold_xtest)\n",
    "            cv_proba = self.format_pred_df(fold_Pa[:,1], self.train_set.loc[test_idx,:])\n",
    "            proba_train = pd.concat([proba_train, cv_proba])\n",
    "        \n",
    "        # Quick way of aggregating cross validation results for all folds\n",
    "        cv_results.append(cross_val_score(classifier, self.Xs_train, self.Y_train, scoring = \"accuracy\", cv = self.generator, n_jobs=-1))\n",
    "        cv_means = np.mean(cv_results)\n",
    "        cv_std = np.std(cv_results)\n",
    "        \n",
    "        # Store cross validation reuslts in dataframe\n",
    "        cv_stats = pd.DataFrame({'Index': self.name, 'Target Var': self.target_var_list[0], 'Method': self.clf[0], \n",
    "                                 \"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std}, index=[0])\n",
    "\n",
    "        cv_stats = cv_stats[['Index', 'Target Var', 'Method', \"CrossValMeans\", \"CrossValerrors\"]]\n",
    "        \n",
    "        return cv_stats, proba_train\n",
    "\n",
    "    \n",
    "    def fit(self, Xs_train, Y_train):\n",
    "        \"\"\"\n",
    "        Fit the model using X as array of features and y as array of labels.\n",
    "        \"\"\"\n",
    "        # Clone does a deep copy of the model in an estimator without actually copying attached data. \n",
    "        # It yields a new estimator with the same parameters that has not been fit on any data.\n",
    "        classifier = clone(self.clf[1])\n",
    "        classifier.fit(Xs_train, Y_train) \n",
    "        return classifier\n",
    "   \n",
    "\n",
    "    def predict_with_one_clf(self, Xs_train, Xs_test, test_set):\n",
    "        \"\"\"\n",
    "        Generate predictions on test set using a classifier.\n",
    "        \"\"\"\n",
    "        classifier = self.fit(Xs_train, self.Y_train)\n",
    "        y_pred = classifier.predict(Xs_test) \n",
    "        y_proba = classifier.predict_proba(Xs_test) \n",
    "        \n",
    "        pred = self.format_pred_df(y_pred, test_set)\n",
    "        proba = self.format_pred_df(y_proba[:,1], test_set)\n",
    "\n",
    "        if self.Y_test != []:\n",
    "            results_stats = calc_classification_stats(self.name, self.target_var_list, self.clf, self.Y_test, y_pred, average='binary')\n",
    "        else:\n",
    "            results_stats = pd.DataFrame()\n",
    " \n",
    "        return results_stats, pred, proba\n",
    "\n",
    "    \n",
    "    def format_pred_df(self, y, dataset):\n",
    "        \"\"\"\n",
    "        Format array of predictions to dataframe\n",
    "        \"\"\"\n",
    "        dataset = dataset[self.index_list+self.target_var_list].reset_index(drop=True)\n",
    "        pred = pd.DataFrame(y, columns=[self.clf[0] + \"_\" + str(self.target_var_list[0])])\n",
    "        dataset = pd.merge(dataset, pred, left_index=True, right_index=True) # Concat indexes to df of predicted results\n",
    "        return dataset\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class containing functions to perform classification on one race with a list of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyOneRace:\n",
    "    \n",
    "    def __init__(self, name, train_set, test_set, Xs_train, Xs_test, Y_train, Y_test, methods, generator=kfold_3, \n",
    "                 index_list=index_list, target_var_list=target_var_list):   \n",
    "        \"\"\"\n",
    "        This is a general purpose class containing functions enabling predictions on an array of features\n",
    "        using a list of classifiers, with classification results and classifier performance statistics\n",
    "        of each classifier stored in dataframes.\n",
    "\n",
    "        methods - A nested list of classifiers. \n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.Xs_train = Xs_train \n",
    "        self.Xs_test = Xs_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        self.methods = methods\n",
    "        self.generator = generator\n",
    "        self.index_list = index_list\n",
    "        self.target_var_list = target_var_list\n",
    "        \n",
    "    def cross_validate_one_race(self):\n",
    "        \"\"\"\n",
    "        Generate cross-validated predictions on train set by iterating through multiple classifiers \n",
    "        \"\"\"\n",
    "        Pa_train_race = pd.DataFrame()\n",
    "        results_all = pd.DataFrame() \n",
    "    \n",
    "        # Iterate through each classifier\n",
    "        for m in self.methods: \n",
    "            # Initialize class to gain access to singular clf specific functions\n",
    "            c1 = ClassifyOneClf(self.name, self.train_set, self.test_set, self.Xs_train, self.Xs_test, self.Y_train, self.Y_test, m)\n",
    "            results, proba_train = c1.cross_validate_one_clf()\n",
    "            results_all = pd.concat([results_all, results]) # Df containing cross-validation performance results of a list of base learners \n",
    "            Pa_train_race = self.format_prediction_matrix(self.train_set, Pa_train_race, proba_train, all_cols=False)\n",
    "        \n",
    "        return results_all, Pa_train_race\n",
    "    \n",
    "    \n",
    "    def predict_one_race(self, fsel_list):\n",
    "        \"\"\"\n",
    "        Generate test set predictions for one race by iterating through multiple classifiers \n",
    "        \"\"\"\n",
    "        Pa_test_race = pd.DataFrame()\n",
    "        P_test_race = pd.DataFrame()     \n",
    "        results_all = pd.DataFrame()\n",
    "        fsel_results_all = pd.DataFrame()\n",
    "    \n",
    "        # Iterate through each classifier\n",
    "        for m in self.methods:\n",
    "            c1 = ClassifyOneClf(self.name, self.train_set, self.test_set, self.Xs_train, self.Xs_test, self.Y_train, self.Y_test, m)\n",
    "            # For linear discriminant analysis, there is a need to ensure variables are not collinear\n",
    "            if (m[0] == \"LDA\"):\n",
    "                df_train_lda, df_test_lda = c1.VIF_filter()\n",
    "                results, pred, proba = c1.predict_with_one_clf(np.array(df_train_lda), np.array(df_test_lda), self.test_set)\n",
    "            if fsel_list!=False:\n",
    "                # Perform feature selection for trees-based classifiers only.\n",
    "                if (m[0] != \"MLP\") and  (m[0] != \"LDA\"):\n",
    "                    fsel_results, Xs_train_sfs, Xs_test_sfs = c1.feature_selection(fsel_list)\n",
    "                    fsel_results_all = pd.concat([fsel_results_all, fsel_results]) # Df containing feature selection results \n",
    "                    results, pred, proba = c1.predict_with_one_clf(Xs_train_sfs, Xs_test_sfs, self.test_set)\n",
    "                else:\n",
    "                    results, pred, proba = c1.predict_with_one_clf(self.Xs_train, self.Xs_test, self.test_set)\n",
    "            else:\n",
    "                results, pred, proba = c1.predict_with_one_clf(self.Xs_train, self.Xs_test, self.test_set)\n",
    "            \n",
    "            results_all = pd.concat([results_all, results]) # Df containing performance statistics of a list of base learners \n",
    "            P_test_race = self.format_prediction_matrix(self.test_set, P_test_race, pred, all_cols=False)\n",
    "            Pa_test_race = self.format_prediction_matrix(self.test_set, Pa_test_race, proba, all_cols=False)\n",
    "\n",
    "        return fsel_results_all, results_all, P_test_race, Pa_test_race\n",
    "    \n",
    "    \n",
    "    def format_prediction_matrix(self, data, P_matrix, pr, all_cols):\n",
    "        \"\"\"\n",
    "        Function to append to a dataframe (P_matrix) each classifier's predictions with each iteration.\n",
    "        \n",
    "        P_matrix: Dataframe containing prediction probabilities\n",
    "        pr: DataFrame containing prediction probabilities fo current \n",
    "        all_cols: True: Merge all original features to matrix of prediction probabilities (May be used for ensembling),\n",
    "                  False: Only index and target variable columns are merged to prediction probabilities\n",
    "        \"\"\"\n",
    "        if (len(P_matrix.columns) == 0):\n",
    "            if all_cols==True:\n",
    "                P_matrix = data.reset_index(drop=True) \n",
    "            else:\n",
    "                P_matrix = data[self.index_list+self.target_var_list].reset_index(drop=True) \n",
    "\n",
    "        P_matrix = pd.merge(P_matrix, pr, on=self.index_list+self.target_var_list, how='left')\n",
    "        return P_matrix\n",
    "         \n",
    "                \n",
    "    def ensemble_stacking(self, top_models_list, Pa_train_race, Pa_test_race, meta_learner, option):\n",
    "        \"\"\"\n",
    "        With the best model known and fixed, this function performs ensemble stacking on one race only.\n",
    "        \n",
    "        Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model.\n",
    "        Often times the stacked model (also called 2nd-level model) will outperform each of the individual models\n",
    "        due its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. \n",
    "        For this reason, stacking is most effective when the base models are significantly different. \n",
    "        \n",
    "        Note that there, is no Y_test (actual target values) as input to this function,\n",
    "        because the stacking is evaluated based on cross-val of train set predictions.\n",
    "        \n",
    "        Option 1: choose number and sequence of models to stack based on correlation with 'best model'.\n",
    "        Option 2: no criteria. will stack all models passed to function.\n",
    "        \"\"\"\n",
    "        # Scope down amount of meta-features ot using in stacking, if required\n",
    "        Pa_train_race = Pa_train_race[self.index_list+self.target_var_list+top_models_list]\n",
    "        Pa_test_race = Pa_test_race[self.index_list+self.target_var_list+top_models_list]\n",
    "            \n",
    "        # Sort rest of models according to correlation of their test predictions with best model\n",
    "        # This is only required for option 1.\n",
    "        if option==1:\n",
    "            best = top_models_list[0]\n",
    "            temp = Pa_test_race.drop(self.index_list+self.target_var_list, axis=1)\n",
    "            cr = pd.DataFrame(temp.corr()[best].sort_values(ascending=True)).reset_index()\n",
    "            a = cr[(cr['index'] == best)]\n",
    "            b = cr[(cr[best] < 0.9) & (cr[best] > 0.4) ]\n",
    "            cr_F = pd.concat([a,b])\n",
    "            sort_models = list(cr_F['index'].values)\n",
    "            Pa_train_race = Pa_train_race[self.index_list+self.target_var_list+sort_models]\n",
    "            Pa_test_race = Pa_test_race[self.index_list+self.target_var_list+sort_models]\n",
    "\n",
    "        # Convert dataframes containing predictions (of train and test set) to arrays\n",
    "        Xs_train_ens = Pa_train_race.drop(self.index_list+self.target_var_list, axis=1)\n",
    "        Xs_test_ens = Pa_test_race.drop(self.index_list+self.target_var_list, axis=1)\n",
    "\n",
    "        if option==1:\n",
    "            # Find the optimal ensemble sequence of meta-features using cross-validation\n",
    "            ensemble_results = self.meta_feature_selection(Pa_train_race, Pa_test_race, Xs_train_ens, Xs_test_ens,\n",
    "                                                           self.Y_train, self.Y_test, meta_learner)\n",
    "            meta_sel_idx = ensemble_results['No. of Base learners'].iloc[0]\n",
    "            \n",
    "            # Only select certain meta-features to use in ensemble\n",
    "            Xs_train_ens = Xs_train_ens.iloc[:,:meta_sel_idx]\n",
    "            Xs_test_ens = Xs_test_ens.iloc[:,:meta_sel_idx]\n",
    "\n",
    "        # Generate predictions from the ensemble\n",
    "        c = ClassifyOneClf(self.name, Pa_train_race, Pa_test_race, Xs_train_ens, Xs_test_ens, self.Y_train, self.Y_test, meta_learner)\n",
    "        results, pred, proba = c.predict_with_one_clf(Xs_train_ens, Xs_test_ens, Pa_test_race)\n",
    "        \n",
    "        if option==2:\n",
    "            ensemble_results = pd.DataFrame()\n",
    "\n",
    "        return ensemble_results, results, pred, proba\n",
    "\n",
    "    \n",
    "    def meta_feature_selection(self, train_set, test_set, Xs_train, Xs_test, Y_train, Y_test, meta_learner):\n",
    "        \"\"\"\n",
    "        This function finds the number of models that gives the best metric score when these models are stacked together.\n",
    "        \"\"\"\n",
    "        ensemble_results = pd.DataFrame()\n",
    "\n",
    "        for i in range(Xs_train.shape[1]):\n",
    "            cv_results = [] \n",
    "            cv_results.append(cross_val_score(meta_learner[1], Xs_train.iloc[:,:i+1], Y_train, \n",
    "                                              scoring = \"roc_auc\", cv = self.generator, n_jobs=-1))\n",
    "            cv_means = np.mean(cv_results)\n",
    "            cv_std = np.std(cv_results)\n",
    "\n",
    "            # Store cross validation reuslts in dataframe\n",
    "            cv_stats = pd.DataFrame({'No. of Base learners': i+1,'Index': self.name, 'Target Var': self.target_var_list[0], \n",
    "                                     'Method': meta_learner[0], \"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std}, index=[0])\n",
    "\n",
    "            cv_stats = cv_stats[['No. of Base learners', 'Index', 'Target Var', 'Method', \"CrossValMeans\", \"CrossValerrors\"]]\n",
    "            ensemble_results = pd.concat([ensemble_results, cv_stats])\n",
    "            ensemble_results.sort_values(\"CrossValMeans\", ascending=False)\n",
    "            \n",
    "        return ensemble_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class containing functions to perform classification on a list of races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyRaces():\n",
    "    \n",
    "    def __init__(self, dfs, dfs_test, methods, generator=kfold_3, races_list=races,\n",
    "                 index_list=index_list, target_var_list=target_var_list, directory=directory): \n",
    "        \"\"\"\n",
    "        This is a class containing functions enabling predictions on a list of races by iterating through each race.\n",
    "        \n",
    "        races_dict - A dictionary of the F1 race calendar (the year of the season and the corresponding races)\n",
    "        \"\"\"\n",
    "        self.dfs = dfs\n",
    "        self.dfs_test = dfs_test \n",
    "        self.methods = methods\n",
    "        self.generator = generator\n",
    "        self.races_dict = races_dict\n",
    "        self.index_list = index_list\n",
    "        self.target_var_list = target_var_list\n",
    "        self.directory = directory\n",
    "        self.races_list = races_list\n",
    "\n",
    "    def run_models(self, actual, metric, qtys, train_yr, test_yr, model_names, fsel_list):\n",
    "        \"\"\"\n",
    "        Iterate through a list of models and races to generate test set predictions for each race with each model.\n",
    "        The last step is to select the best performing model by comparing a fixed metric of test results for each model\n",
    "        Note: Actual test target values are known in selecting the best model. \n",
    "        \n",
    "        actual - Is the actual target variable values known? If yes -> actual=False. If no -> actual=True\n",
    "        metric - Chosen metric to measure and compare models by\n",
    "        qtys - list of values to input for 'qty_races_tofilter' parameter when building train-test set\n",
    "        model_names - list of strings to index each model\n",
    "        \"\"\"\n",
    "        Pa_train_all = pd.DataFrame()\n",
    "        Pa_test_all = pd.DataFrame()\n",
    "        df_list = []\n",
    "        all_report = pd.DataFrame() \n",
    "        fsel_results_all = pd.DataFrame()\n",
    "        cv_results_all = pd.DataFrame()\n",
    "        results_all = pd.DataFrame()\n",
    "        \n",
    "        for name in self.races_list:\n",
    "\n",
    "            Pa_train_ensem = pd.DataFrame()  \n",
    "            Pa_test_ensem = pd.DataFrame()\n",
    "            \n",
    "            models = []\n",
    "            for i,j in itertools.product(zip(self.dfs,self.dfs_test), qtys):\n",
    "                models.append([i[0], i[1], j])\n",
    "    \n",
    "            for idx, i in enumerate(models):\n",
    "                # Initiate class \n",
    "                b = Build_train_test_set(actual=False, df=i[0], df_test=i[1], name=name)\n",
    "\n",
    "                # Create training and test sets\n",
    "                train_set, test_set, Xs_train, Xs_test, Y_train, Y_test = b.train_test_split(train_yr, test_yr, i[2])\n",
    "\n",
    "                # Initiate class\n",
    "                c = ClassifyOneRace(name, train_set, test_set, Xs_train, Xs_test, Y_train, Y_test, self.methods)\n",
    "\n",
    "                # Generate cross-validated predictions of train set from base-learners\n",
    "                cv_results, Pa_train_race = c.cross_validate_one_race()\n",
    "                cv_results['Model'] = model_names[idx]\n",
    "                \n",
    "                # Generate first-level predictions of test set\n",
    "                fsel_results, results, P_test_race, Pa_test_race = c.predict_one_race(fsel_list)\n",
    "                fsel_results['Model'] = model_names[idx]\n",
    "                results['Model'] = model_names[idx]\n",
    "                \n",
    "                # Suffix model name to train and test columns with predictions\n",
    "                rename_some_cols(Pa_train_race, model_names[idx], col_start=len(self.index_list+self.target_var_list))\n",
    "                rename_some_cols(Pa_test_race, model_names[idx], col_start=len(self.index_list+self.target_var_list))\n",
    "                \n",
    "                # Merge the predictions of each model horizontally\n",
    "                Pa_train_ensem = c.format_prediction_matrix(train_set, Pa_train_ensem, Pa_train_race, all_cols=False)\n",
    "                Pa_test_ensem = c.format_prediction_matrix(test_set, Pa_test_ensem, Pa_test_race, all_cols=False)                       \n",
    "                \n",
    "                # Concatenate the following dfs:\n",
    "                    #1) train set cross-validation results, if any\n",
    "                    #2) test results of each MODEL and each RACE\n",
    "                    #3) feature selection results, if any\n",
    "                fsel_results_all = pd.concat([fsel_results_all, fsel_results])\n",
    "                cv_results_all = pd.concat([cv_results_all, cv_results])\n",
    "                results_all = pd.concat([results_all, results])\n",
    "                \n",
    "            # Merge the predictions of each race vertically\n",
    "            Pa_train_all = pd.concat([Pa_train_all, Pa_train_ensem]) # Prediction probabilities generated from cross-validated train set \n",
    "            Pa_test_all = pd.concat([Pa_test_all, Pa_test_ensem])\n",
    "        \n",
    "        Pa_train_all.to_csv(\"Pa_train.csv\", index = False)\n",
    "        Pa_test_all.to_csv(\"Pa_test.csv\", index = False)\n",
    "        cv_results_all.to_csv(\"cv_results.csv\", index = False)\n",
    "        \n",
    "        # Based on function's settings, save and return required dfs accordingly\n",
    "        if actual==False:\n",
    "\n",
    "            # Find the best peforming model\n",
    "            m_report = pd.DataFrame()\n",
    "            for i,j in results_all.groupby(['Model']):\n",
    "                r = model_report(i, j)\n",
    "                m_report = pd.concat([m_report, r])\n",
    "            m_report.sort_values('Average Dist from baseline', ascending=False)\n",
    "\n",
    "            # Find the best peforming sub-model (Each race must use the same best performing sub-model)\n",
    "            report = pd.DataFrame(results_all.groupby(['Model','Method'])[metric].agg(\"mean\")).reset_index()\n",
    "            report = report.sort_values(metric, ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            m_report.to_csv(\"model_report.csv\", index = False)\n",
    "            report.to_csv(\"submodel_report.csv\", index = False)\n",
    "            results_all.to_csv(\"results.csv\", index = False)\n",
    "            return m_report, report, Pa_train_all, Pa_test_all, cv_results_all, results_all\n",
    "        \n",
    "        if fsel_list==True:\n",
    "            fsel_results_all.to_csv(\"fsel_results.csv\", index = False)\n",
    "            return m_report, report, Pa_train_all, Pa_test_all, cv_results_all, results_all, fsel_results_all\n",
    "\n",
    "        return Pa_train_all, Pa_test_all, cv_results_all\n",
    "    \n",
    "    \n",
    "    def ensemble_stacking(self, report, train_yr, test_yr, Pa_train_all, Pa_test_all, meta_learner, option):\n",
    "        \"\"\"\n",
    "        With the best model known and fixed, this function performs ensemble stacking by iterating through a list of races.\n",
    "        \n",
    "        In actual test prediction conditions (ie. meaning pre-race, when the race results are not known),\n",
    "        it is not required to loop through the list of races since our goal is just to predict the finish statuses of ONE RACE.\n",
    "        However, because in the model-building phase, I would like to discover how ensemble stacking will work with a variety of races, I will repeat the steps for each race\n",
    "        \n",
    "        report - datafame containing performance results (metrics) of each classifier \n",
    "        meta_learner - Chosen meta-learner formatted in a list. eg. [['Gradient Boosting', GradientBoostingClassifier()]]\n",
    "        \"\"\"\n",
    "        pred_all = pd.DataFrame()\n",
    "        proba_all = pd.DataFrame()\n",
    "        results_all = pd.DataFrame()\n",
    "        ensemble_results_all = pd.DataFrame()      \n",
    "        \n",
    "        report['column names'] = report['Method'] + '_statusId_' + report['Model']\n",
    "        top_models_list = list(report['column names'])\n",
    "\n",
    "        for name in self.races_list:\n",
    "        \n",
    "            # Create training and test sets of first-level prediction for each race\n",
    "            b = Build_train_test_set(actual=True, df=Pa_train_all, df_test=Pa_test_all, name=name)\n",
    "            Pa_train_race, Pa_test_race, Xs_train, Xs_test, Y_train, Y_test = b.train_test_split(train_yr, test_yr, int(top_models_list[0][-1]))\n",
    "\n",
    "            #Pa_train_race = self.aggregate_pred_proba(Pa_train_race)\n",
    "            \n",
    "            # Perform stacking on one race only\n",
    "            c = ClassifyOneRace(name=name, train_set=\"\", test_set=\"\", Xs_train=\"\", Xs_test=\"\", Y_train=Y_train, Y_test=Y_test, methods=\"\")\n",
    "            \n",
    "            ensemble_results, results, pred, proba = c.ensemble_stacking(top_models_list, Pa_train_race, Pa_test_race, meta_learner, option)\n",
    "\n",
    "            ensemble_results_all = pd.concat([ensemble_results_all, ensemble_results]) # Dataframe contatining metrics of each clf performance\n",
    "            results_all = pd.concat([results_all, results]) # Dataframe contatining metrics of each clf performance\n",
    "            proba_all = pd.concat([proba_all, proba]) # Prediction probabilities generated from test set\n",
    "            pred_all = pd.concat([pred_all, pred]) # Predictions generated from test set\n",
    "        \n",
    "        if option==1:\n",
    "            return ensemble_results_all, results_all, pred_all, proba_all\n",
    "        else:\n",
    "            return results_all, pred_all, proba_all\n",
    "\n",
    "\n",
    "    def aggregate_pred_proba(self, proba_train_all):      \n",
    "\n",
    "        # Because there are about 20 races in a season, depending on the train size, a race may appear multiple times.\n",
    "        # For eg if qty_races_tofilter=5, within a season, the same race will be appear in 6 different train sets.\n",
    "        # Hence, aggregate the prediction probabilities for the race for each classifier.\n",
    "        proba_train_all = proba_train_all.groupby(self.index_list+self.target_var_list).agg({'mean'})\n",
    "        proba_train_all.columns = [col[0] for col in proba_train_all.columns]\n",
    "        return proba_train_all.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellenous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_some_cols(df, suffix, col_start):\n",
    "    new_names = [(i,i+\"_\"+suffix) for i in df.iloc[:, col_start:].columns.values]\n",
    "    return df.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "def model_report(model_name, df):\n",
    "    abv_baseline = df[df['Distance from baseline'] > 0]\n",
    "    report = pd.DataFrame(abv_baseline.Method.value_counts()).T\n",
    "    report['No. of races: Test acc > Baseline acc'] = len(abv_baseline.Index.unique())\n",
    "    report['List of races: Test acc > Baseline acc'] = [abv_baseline.Index.unique()]\n",
    "    report['Average Avg Precision'] = df['Avg Precision'].agg(\"mean\")\n",
    "    report['Average F1 score'] = df['F1 Score'].agg(\"mean\")\n",
    "    report['Average AUC score'] = df['AUC Score'].agg(\"mean\")\n",
    "    report['Average Dist from baseline'] = df['Distance from baseline'].agg(\"mean\")\n",
    "    report.rename(index={'Method': model_name}, inplace=True)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to calculate or plot classification statistics / results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Plotting Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve_loop(df, df_test, clfs, train_yr, test_yr, races_to_plot, generator, train_sizes, qty_races_tofilter):\n",
    "    \"\"\"\n",
    "    Function to loop through a list of classifiers and races and plot learning curve for each iteration.\n",
    "    \"\"\"\n",
    "    nrows = len(clfs)\n",
    "    ncols = len(races_to_plot)\n",
    "\n",
    "    cols = [i for i in races_to_plot]\n",
    "    rows = [i[0] for i in clfs]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15,15), sharex=True, sharey=True)\n",
    "\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "\n",
    "            b = Build_train_test_set(True, df, df_test, races_to_plot[col])\n",
    "            train_set, test_set, Xs_train, Xs_test, Y_train, Y_test = b.train_test_split(train_yr, test_yr, qty_races_tofilter)\n",
    "\n",
    "            plot_learning_curve(axes, row, col, clfs[row][1], Xs_train, Y_train, generator, train_sizes)\n",
    "\n",
    "    for ax, col in zip(axes[0], cols):\n",
    "        ax.set_title(col)\n",
    "\n",
    "    for ax, row in zip(axes[:,0], rows):\n",
    "        ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "\n",
    "def plot_learning_curve(axes, row, col, estimator, X, y, cv, train_sizes):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve\n",
    "    \"\"\"\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=-1,\n",
    "                                                            train_sizes=train_sizes, scoring='accuracy')\n",
    "    train_scores_mean = 1-np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = 1-np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    axes[row,col].grid()\n",
    "\n",
    "    axes[row,col].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    axes[row,col].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    axes[row,col].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training error\")\n",
    "    axes[row,col].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation error\")\n",
    "\n",
    "    axes[row,col].legend(loc=\"best\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Calculate metrics of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_classification_stats(race_name, target_var_list, classifier, y_test, y_pred, average):\n",
    "    \"\"\"\n",
    "    Create dataframe containing metrics of classification results.\n",
    "\n",
    "    y_test - Actual target variable values\n",
    "    y_pred - Predicted target variable values\n",
    "    average - This is a paramaeter for sklearn's f1 score metric\n",
    "    \"\"\"\n",
    "    baseline_accuracy = []\n",
    "    test_accuracy = []\n",
    "    f1 = []\n",
    "    avg_precision = []\n",
    "    auc = []\n",
    "\n",
    "    baseline_accuracy.append(float(pd.Series(y_test).value_counts().max()) / pd.Series(y_test).count())\n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    avg_precision.append(average_precision_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average=average)) \n",
    "    auc.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    results_stats = pd.DataFrame({'Index': race_name, 'Target Var': target_var_list[0], 'Method': classifier[0],\n",
    "                                  'Avg Precision': avg_precision, 'F1 Score': f1, \"AUC Score\": auc,\n",
    "                                  'Test accuracy': test_accuracy, 'Baseline accuracy': baseline_accuracy})\n",
    "\n",
    "    results_stats['Distance from baseline'] = results_stats['Test accuracy'] - results_stats['Baseline accuracy']\n",
    "\n",
    "    results_stats = results_stats[['Index', 'Target Var', 'Method', \"Test accuracy\", 'Distance from baseline',\n",
    "                                   'AUC Score', 'F1 Score', 'Avg Precision']]\n",
    "\n",
    "    return results_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Plot metrics of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_algo_results(df, grp_col, metrics_list, sort_method):   \n",
    "    \"\"\"\n",
    "    Function to plot statistics of prediction results\n",
    "    \"\"\"\n",
    "    def calc(df, grp_col, col, new_col_name):\n",
    "        if new_col_name == 'Std':\n",
    "            df = pd.DataFrame(df.groupby([grp_col])[col].apply(lambda x: np.std(x)))\n",
    "  \n",
    "        elif new_col_name == 'Mean':\n",
    "            df = pd.DataFrame(df.groupby([grp_col])[col].apply(lambda x: np.mean(x)))\n",
    "            \n",
    "        return df.reset_index().rename(columns={col: col+ \" (\" + new_col_name + \")\"})\n",
    "    \n",
    "    # This function only calculates mean and standard deviation\n",
    "    def create_grp_stats(df, grp_col, col):\n",
    "        p_std = calc(df, grp_col, col, 'Std')\n",
    "        p_mean = calc(df, grp_col, col, 'Mean')\n",
    "        p = pd.merge(p_mean, p_std, on=grp_col, how='left')\n",
    "        return p\n",
    "    \n",
    "    # Set order \n",
    "    def sort_order(df, col_to_sortby, grp_col):\n",
    "        if sort_method == \"desc\":\n",
    "            ordering = df.sort_values([col_to_sortby])[grp_col].unique()\n",
    "        elif sort_method == \"race\":\n",
    "            ordering = races\n",
    "        else:\n",
    "            raise ValueError(\"Only desc or race are accepted keywords for sort_method variable\")\n",
    "\n",
    "        ids = reversed(list(ordering))\n",
    "        ids = [str(item) for item in ids]  \n",
    "        return ids\n",
    "        \n",
    "    # plot results\n",
    "    def plot_barplot(df, x, y, ids, row, col):\n",
    "        plt.figure()   \n",
    "        g = sns.barplot(x, y, data = df, order=ids, palette=\"Set3\", orient = \"h\", ax=axes[row][col])\n",
    "        g.set_title(x, fontsize=16)\n",
    "        for p in g.patches:\n",
    "            width = p.get_width()\n",
    "            g.text(width*1.05, p.get_y()+0.55*p.get_height(), '{:1.2f}'.format(width), ha='center', va='center')\n",
    "        \n",
    "    df_new = pd.DataFrame()\n",
    "    for i in metrics_list:\n",
    "        p = create_grp_stats(df, grp_col, i) \n",
    "        df_new = pd.concat([df_new, p], axis=1)\n",
    "        df_new = df_new.T.drop_duplicates().T\n",
    "    \n",
    "    if len(df_new.columns) == 3:\n",
    "        nrows = 1\n",
    "        ncols = 2\n",
    "    else:\n",
    "        nrows = len(df_new.columns)-len(metrics_list)-1\n",
    "        ncols = 2\n",
    "\n",
    "    to_plot = df_new.columns[1:]\n",
    "    fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15), squeeze=False)\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    counter = 0\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            ids = sort_order(df_new, to_plot[counter], grp_col)\n",
    "            plot_barplot(df_new, to_plot[counter], grp_col, ids, row, col)\n",
    "            counter += 1\n",
    "    plt.tight_layout()\n",
    "            \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Plot ensemble sequence results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ensemble_seq(df):\n",
    "    \"\"\"\n",
    "    Function to plot number and sequence of models chosen to be stacked for each race\n",
    "    \n",
    "    df -  dataframe of ensemble results \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(30,20))\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.set_palette(\"hls\", 20)\n",
    "\n",
    "    # Put the legend out of the figure\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    sns.pointplot(x=\"No. of Base learners\", y=\"CrossValMeans\", hue=\"Index\", data=df)\n",
    "\n",
    "    plt.figure(figsize=(10,20))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.barplot(x=\"CrossValMeans\", y=\"Index\", data=df, hue='No. of Base learners', palette='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conf_mat(df_prob, idx, col_labels):   \n",
    "    \"\"\"\n",
    "    Function to calculate confusion matrix for a chosen model.\n",
    "    \n",
    "    df_prob - dataframe containing a column of prediction probabilites and another column of actual target values\n",
    "    idx - list of class labels (eg. [DNF, FIN])\n",
    "    col_labels - list of class labels suffixed with model name (eg. [DNF_A, FIN_A])\n",
    "    \"\"\"\n",
    "    conf_arr = np.zeros(shape=(2,2))\n",
    "    driver_arr = np.array([[0, 0], [0, 0]], dtype=object)\n",
    "    drivers_FP = []\n",
    "    drivers_TN = []\n",
    "    drivers_TP = []\n",
    "    drivers_FN = []\n",
    "\n",
    "    prob_arr = np.array(df_prob.iloc[:,-1])\n",
    "    input_arr = np.array(df_prob.statusId)\n",
    "\n",
    "    df_prob = df_prob.reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(prob_arr)):\n",
    "        if int(input_arr[i]) == 0:\n",
    "            if float(prob_arr[i]) > 0.5:\n",
    "                # Predicted No, Actual Yes\n",
    "                conf_arr[0][1] = conf_arr[0][1] + 1\n",
    "                drivers_FP.append(df_prob.loc[i, 'driverRef'])\n",
    "                driver_arr[0][1] = drivers_FP # Store list with driver names that match criteria\n",
    "            else:\n",
    "                # Predicted No, Actual No\n",
    "                conf_arr[0][0] = conf_arr[0][0] + 1\n",
    "                drivers_TN.append(df_prob.loc[i, 'driverRef'])\n",
    "                driver_arr[0][0] =  drivers_TN\n",
    "        elif int(input_arr[i]) == 1:\n",
    "            if float(prob_arr[i]) <= 0.5:\n",
    "                # Predicted Yes, Actual No\n",
    "                conf_arr[1][0] = conf_arr[1][0] +1\n",
    "                drivers_FN.append(df_prob.loc[i, 'driverRef'])\n",
    "                driver_arr[1][0] = drivers_FN\n",
    "            else:\n",
    "                # Predicted Yes, Actual Yes\n",
    "                conf_arr[1][1] = conf_arr[1][1] +1\n",
    "                drivers_TP.append(df_prob.loc[i, 'driverRef'])\n",
    "                driver_arr[1][1] =  drivers_TP\n",
    "    \n",
    "    # Convert confusion matrix to percentages\n",
    "    #cm_sum = np.sum(conf_arr)\n",
    "    #cm_perc = cm / cm_sum.astype(float) * 100\n",
    "\n",
    "    conf_matrix = pd.DataFrame(conf_arr, index=idx, columns=col_labels)\n",
    "    driver_matrix = pd.DataFrame(driver_arr, index=idx, columns=col_labels)\n",
    "        \n",
    "    return conf_matrix, driver_matrix\n",
    "\n",
    "def conf_mat_each_race(df_prob, labels, model_name):\n",
    "    \"\"\"\n",
    "    Function to calculate confusion matrix for each race and returns the drivers that belong to each quarter of the matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix_all = pd.DataFrame()\n",
    "    driver_matrix_all = pd.DataFrame()\n",
    "\n",
    "    for name, group in df_prob.groupby(['year', 'name']):\n",
    "        df_prob_grp = df_prob[df_prob[['year', 'name']].apply(tuple, 1).isin([name])]\n",
    "        \n",
    "        idx = pd.MultiIndex.from_product([[name], labels], names=['race', 'status'])\n",
    "        col_labels = [i+model_name for i in labels]\n",
    "        \n",
    "        conf_matrix, driver_matrix = conf_mat(df_prob_grp, idx, col_labels)\n",
    "        \n",
    "        # Plot confusion matrix as a heatmap with drivers' names in the annotation\n",
    "        #fig, ax = plt.subplots(figsize=(4,4))\n",
    "        #sns.heatmap(conf_matrix, annot=np.array(driver_matrix), fmt = '')\n",
    "        \n",
    "        conf_matrix_all = pd.concat([conf_matrix_all, conf_matrix])\n",
    "        driver_matrix_all = pd.concat([driver_matrix_all, driver_matrix])\n",
    "        \n",
    "    return conf_matrix_all, driver_matrix_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
